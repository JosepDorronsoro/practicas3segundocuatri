{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISVG5s5HGJCQ"
   },
   "source": [
    "# SIMD Autovectorization in Numba\n",
    "\n",
    "<span style=\"color: red\">**WARNING**</span>: *Due to CPU limitations on Binder, not all the benefits of SIMD instructions will be visible.  You will see better SIMD performance if you download this notebook and run on Intel Haswell / AMD Zen or later.*\n",
    "\n",
    "Most modern CPUs have support for instructions that apply the same operation to multiple data elements simultaneously.  These are called \"Single Instruction, Multiple Data\" (SIMD) operations, and the LLVM backend used by Numba can generate them in some cases to execute loops more quickly.  (This process is called \"autovectorization.\")\n",
    "\n",
    "For example, Intel processors have support for SIMD instruction sets like:\n",
    "\n",
    "* SSE (128-bit inputs)\n",
    "* AVX (256-bit inputs)\n",
    "* AVX-512 (512-bit inputs, Skylake-X and later or Xeon Phi)\n",
    "\n",
    "These wide instructions typically operate on as many values as will fit into an input register.  For AVX instructions, this means that either 8 float32 values or 4 float64 values can be processed as a single input.  As a result, the NumPy dtype that you use can potentially impact performance to a greater degree than when SIMD is not in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kXNITK9YGJCW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMq7r5HEGJCY"
   },
   "source": [
    "It can be somewhat tricky to determine when LLVM has successfully autovectorized a loop.  The Numba team is working on exporting diagnostic information to show where the autovectorizer has generated SIMD code.  For now, we can use a fairly crude approach of searching the assembly language generated by LLVM for SIMD instructions.\n",
    "\n",
    "It is also interesting to note what kind of SIMD is used on your system.  On x86_64, the name of the registers used indicates which level of SIMD is in use:\n",
    "\n",
    "* SSE: `xmmX`\n",
    "* AVX/AVX2: `ymmX`\n",
    "* AVX-512: `zmmX`\n",
    "\n",
    "where X is an integer.\n",
    "\n",
    "**Note**: The method we use below to find SIMD instructions will only work on Intel/AMD CPUs.  Other platforms have entirely different assembly language syntax for SIMD instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7-hXbLoPGJCY"
   },
   "outputs": [],
   "source": [
    "def find_instr(func, keyword, sig=0, limit=5):\n",
    "    count = 0\n",
    "    for l in func.inspect_asm(func.signatures[sig]).split('\\n'):\n",
    "        if keyword in l:\n",
    "            count += 1\n",
    "            print(l)\n",
    "            if count >= limit:\n",
    "                break\n",
    "    if count == 0:\n",
    "        print('No instructions found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZcnfzlbGJCZ"
   },
   "source": [
    "## Basic SIMD\n",
    "\n",
    "Let's start with a simple function that returns the square difference between two arrays, as you might write for a least-squares optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BnTlZs64GJCZ"
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def sqdiff(x, y):\n",
    "    out = np.empty_like(x)\n",
    "    for i in range(x.shape[0]):\n",
    "        out[i] = (x[i] - y[i])**2\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wQwYjJi5GJCa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.99999976, 1.        , ..., 1.        , 1.0000002 ,\n",
       "       1.        ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x32 = np.linspace(1, 2, 10000, dtype=np.float32)\n",
    "y32 = np.linspace(2, 3, 10000, dtype=np.float32)\n",
    "sqdiff(x32, y32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "R8fDOyOxGJCa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.99999976, 1.        , ..., 1.        , 1.00000024,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x64 = x32.astype(np.float64)\n",
    "y64 = y32.astype(np.float64)\n",
    "sqdiff(x64, y64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7UWeYvMGJCb"
   },
   "source": [
    "Numba has created two different implementations of the function, one for `float32` 1-D arrays, and one for `float64` 1-D arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QkdvHwJRGJCb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Array(float32, 1, 'C', False, aligned=True),\n",
       "  Array(float32, 1, 'C', False, aligned=True)),\n",
       " (Array(float64, 1, 'C', False, aligned=True),\n",
       "  Array(float64, 1, 'C', False, aligned=True))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqdiff.signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCLLOMJBGJCc"
   },
   "source": [
    "This allows Numba (and LLVM) to specialize the use of the SIMD instructions for each situation.  In particular, using lower precision floating point allows twice as many values to fit into a SIMD register.  We will see that for the same number of elements, the `float32` calculation goes twice as fast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rTCx6wv6GJCc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "909 ns ± 23.3 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "2.14 μs ± 47 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sqdiff(x32, y32)\n",
    "%timeit sqdiff(x64, y64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLyQv1czGJCc"
   },
   "source": [
    "We can check for SIMD instructions in both cases.  (Due to the order of compilation above, signature 0 is the `float32` implementation and signature 1 is the `float64` implementation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sWSNL7krGJCc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32:\n",
      "\tfsub\ts0, s0, s1\n",
      "\tfsub.4s\tv0, v0, v4\n",
      "\tfsub.4s\tv1, v1, v5\n",
      "\tfsub.4s\tv2, v2, v6\n",
      "\tfsub.4s\tv3, v3, v7\n",
      "---\n",
      "float64:\n",
      "\tfsub\td0, d0, d1\n",
      "\tfsub.2d\tv0, v0, v4\n",
      "\tfsub.2d\tv1, v1, v5\n",
      "\tfsub.2d\tv2, v2, v6\n",
      "\tfsub.2d\tv3, v3, v7\n"
     ]
    }
   ],
   "source": [
    "print('float32:')\n",
    "find_instr(sqdiff, keyword='fsub', sig=0) # Notación para arquitectura M4 \n",
    "print('---\\nfloat64:')\n",
    "find_instr(sqdiff, keyword='fsub', sig=1) # Notación para arquitectura M4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIw1xGJWGJCd"
   },
   "source": [
    "In x86_64 assembly, SSE uses `subps` for \"subtraction packed single precision\" (AVX uses `vsubps`), representing vector float32 operations.  The `subpd` instruction (AVX = `vsubpd`) stands for \"subtraction packed double precision\", representing float64 operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nr3KG_LwGJCd"
   },
   "source": [
    "## SIMD and Division\n",
    "\n",
    "In general, the autovectorizer cannot deal with branches inside loops, although this is an area where LLVM is likely to improve in the future.  Your best bet for SIMD acceleration is to only have pure math operations in the loop.\n",
    "\n",
    "As a result, you would naturally assume a function like this would be OK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "McQdCtH7GJCd"
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def frac_diff1(x, y):\n",
    "    out = np.empty_like(x)\n",
    "    for i in range(x.shape[0]):\n",
    "        out[i] = 2 * (x[i] - y[i]) / (x[i] + y[i])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Fbt6Zs9KGJCd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6666667 , -0.66662216, -0.66657776, ..., -0.400032  ,\n",
       "       -0.40001604, -0.4       ], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frac_diff1(x32, y32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5jA0tR8cGJCd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tfsub\ts0, s0, s1\n"
     ]
    }
   ],
   "source": [
    "find_instr(frac_diff1, keyword='fsub', sig=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kS-DcAXaGJCd"
   },
   "source": [
    "`No instructions found`?!\n",
    "\n",
    "The problem is that division by zero can behave in two different ways:\n",
    "\n",
    "* In Python, division by zero raises an exception.\n",
    "* In NumPy, division by zero results in a `NaN`, like in C.\n",
    "\n",
    "By default, Numba `@jit` follows the Python convention, and `@vectorize`/`@guvectorize` follow the NumPy convention.  When following the Python convention, a simple division operation `r = x / y` expands out into something like:\n",
    "\n",
    "``` python\n",
    "\n",
    "if y == 0:\n",
    "    raise ZeroDivisionError()\n",
    "else:\n",
    "    r = x / y\n",
    "```\n",
    "\n",
    "This branching code causes the autovectorizer to give up, and no SIMD to be generated for our example above.\n",
    "\n",
    "Fortunately, Numba allows you to override the \"error model\" of the function if you don't want a `ZeroDivisionError` to be raised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZAEpHoKfGJCd"
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True, error_model='numpy')\n",
    "def frac_diff2(x, y):\n",
    "    out = np.empty_like(x)\n",
    "    for i in range(x.shape[0]):\n",
    "        out[i] = 2 * (x[i] - y[i]) / (x[i] + y[i])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xn0lJSE3GJCe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6666667 , -0.66662216, -0.66657776, ..., -0.400032  ,\n",
       "       -0.40001604, -0.4       ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frac_diff2(x32, y32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vYT6MnxfGJCe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tfsub\ts2, s0, s1\n",
      "\tfsub.4s\tv2, v0, v1\n"
     ]
    }
   ],
   "source": [
    "find_instr(frac_diff2, keyword='fsub', sig=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3g-jLiboGJCe"
   },
   "source": [
    "We have SIMD instructions again, but when we check the speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Z4BEiTbOGJCe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.01 μs ± 143 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      "2.22 μs ± 55.4 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit frac_diff2(x32, y32)\n",
    "%timeit frac_diff2(x64, y64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlS9G0sIGJCe"
   },
   "source": [
    "This is faster than the no-SIMD case, but there doesn't seem to be a speed benefit with `float32` inputs.  What's going on?\n",
    "\n",
    "The remaining issue is very subtle.  We can see it if we look at a type-annotated version of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zt1flREYGJCe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frac_diff2 (Array(float32, 1, 'C', False, aligned=True), Array(float32, 1, 'C', False, aligned=True))\n",
      "--------------------------------------------------------------------------------\n",
      "# File: /var/folders/65/r55xnbt14k5fgbrtf11p9g4m0000gp/T/ipykernel_6315/2090011437.py\n",
      "# --- LINE 1 --- \n",
      "# label 0\n",
      "#   x = arg(0, name=x)  :: array(float32, 1d, C)\n",
      "#   y = arg(1, name=y)  :: array(float32, 1d, C)\n",
      "\n",
      "@jit(nopython=True, error_model='numpy')\n",
      "\n",
      "# --- LINE 2 --- \n",
      "\n",
      "def frac_diff2(x, y):\n",
      "\n",
      "    # --- LINE 3 --- \n",
      "    #   $4load_global.0 = global(np: <module 'numpy' from '/Users/ivandominguez/anaconda3/lib/python3.11/site-packages/numpy/__init__.py'>)  :: Module(<module 'numpy' from '/Users/ivandominguez/anaconda3/lib/python3.11/site-packages/numpy/__init__.py'>)\n",
      "    #   $16load_method.2 = getattr(value=$4load_global.0, attr=empty_like)  :: Function(<built-in function empty_like>)\n",
      "    #   del $4load_global.0\n",
      "    #   out = call $16load_method.2(x, func=$16load_method.2, args=[Var(x, 2090011437.py:1)], kws=(), vararg=None, varkwarg=None, target=None)  :: (Array(float32, 1, 'C', False, aligned=True), omitted(default=None)) -> array(float32, 1d, C)\n",
      "    #   del $16load_method.2\n",
      "\n",
      "    out = np.empty_like(x)\n",
      "\n",
      "    # --- LINE 4 --- \n",
      "    #   $56load_global.5 = global(range: <class 'range'>)  :: Function(<class 'range'>)\n",
      "    #   $70load_attr.8 = getattr(value=x, attr=shape)  :: UniTuple(int64 x 1)\n",
      "    #   $const80.9.1 = const(int, 0)  :: Literal[int](0)\n",
      "    #   $82binary_subscr.10 = static_getitem(value=$70load_attr.8, index=0, index_var=$const80.9.1, fn=<built-in function getitem>)  :: int64\n",
      "    #   del $const80.9.1\n",
      "    #   del $70load_attr.8\n",
      "    #   $96call.11 = call $56load_global.5($82binary_subscr.10, func=$56load_global.5, args=[Var($82binary_subscr.10, 2090011437.py:4)], kws=(), vararg=None, varkwarg=None, target=None)  :: (int64,) -> range_state_int64\n",
      "    #   del $82binary_subscr.10\n",
      "    #   del $56load_global.5\n",
      "    #   $106get_iter.12 = getiter(value=$96call.11)  :: range_iter_int64\n",
      "    #   del $96call.11\n",
      "    #   $phi108.0 = $106get_iter.12  :: range_iter_int64\n",
      "    #   del $106get_iter.12\n",
      "    #   jump 108\n",
      "    # label 108\n",
      "    #   $108for_iter.1 = iternext(value=$phi108.0)  :: pair<int64, bool>\n",
      "    #   $108for_iter.2 = pair_first(value=$108for_iter.1)  :: int64\n",
      "    #   $108for_iter.3 = pair_second(value=$108for_iter.1)  :: bool\n",
      "    #   del $108for_iter.1\n",
      "    #   $phi110.1 = $108for_iter.2  :: int64\n",
      "    #   del $108for_iter.2\n",
      "    #   branch $108for_iter.3, 110, 196\n",
      "    # label 110\n",
      "    #   del $108for_iter.3\n",
      "    #   i = $phi110.1  :: int64\n",
      "    #   del $phi110.1\n",
      "\n",
      "    for i in range(x.shape[0]):\n",
      "\n",
      "        # --- LINE 5 --- \n",
      "        #   $const112.2.2 = const(int, 2)  :: Literal[int](2)\n",
      "        #   $118binary_subscr.5 = getitem(value=x, index=i, fn=<built-in function getitem>)  :: float32\n",
      "        #   $132binary_subscr.8 = getitem(value=y, index=i, fn=<built-in function getitem>)  :: float32\n",
      "        #   $binop_sub142.9 = $118binary_subscr.5 - $132binary_subscr.8  :: float32\n",
      "        #   del $132binary_subscr.8\n",
      "        #   del $118binary_subscr.5\n",
      "        #   $binop_mul146.10 = $const112.2.2 * $binop_sub142.9  :: float64\n",
      "        #   del $const112.2.2\n",
      "        #   del $binop_sub142.9\n",
      "        #   $154binary_subscr.13 = getitem(value=x, index=i, fn=<built-in function getitem>)  :: float32\n",
      "        #   $168binary_subscr.16 = getitem(value=y, index=i, fn=<built-in function getitem>)  :: float32\n",
      "        #   $binop_add178.17 = $154binary_subscr.13 + $168binary_subscr.16  :: float32\n",
      "        #   del $168binary_subscr.16\n",
      "        #   del $154binary_subscr.13\n",
      "        #   $binop_truediv182.18 = $binop_mul146.10 / $binop_add178.17  :: float64\n",
      "        #   del $binop_mul146.10\n",
      "        #   del $binop_add178.17\n",
      "        #   out[i] = $binop_truediv182.18  :: (Array(float32, 1, 'C', False, aligned=True), int64, float64) -> none\n",
      "        #   del i\n",
      "        #   del $binop_truediv182.18\n",
      "        #   jump 108\n",
      "\n",
      "        out[i] = 2 * (x[i] - y[i]) / (x[i] + y[i])\n",
      "\n",
      "    # --- LINE 6 --- \n",
      "    # label 196\n",
      "    #   del y\n",
      "    #   del x\n",
      "    #   del $phi110.1\n",
      "    #   del $phi108.0\n",
      "    #   del $108for_iter.3\n",
      "    #   $198return_value.1 = cast(value=out)  :: array(float32, 1d, C)\n",
      "    #   del out\n",
      "    #   return $198return_value.1\n",
      "\n",
      "    return out\n",
      "\n",
      "\n",
      "================================================================================\n",
      "frac_diff2 (Array(float64, 1, 'C', False, aligned=True), Array(float64, 1, 'C', False, aligned=True))\n",
      "--------------------------------------------------------------------------------\n",
      "# File: /var/folders/65/r55xnbt14k5fgbrtf11p9g4m0000gp/T/ipykernel_6315/2090011437.py\n",
      "# --- LINE 1 --- \n",
      "# label 0\n",
      "#   x = arg(0, name=x)  :: array(float64, 1d, C)\n",
      "#   y = arg(1, name=y)  :: array(float64, 1d, C)\n",
      "\n",
      "@jit(nopython=True, error_model='numpy')\n",
      "\n",
      "# --- LINE 2 --- \n",
      "\n",
      "def frac_diff2(x, y):\n",
      "\n",
      "    # --- LINE 3 --- \n",
      "    #   $4load_global.0 = global(np: <module 'numpy' from '/Users/ivandominguez/anaconda3/lib/python3.11/site-packages/numpy/__init__.py'>)  :: Module(<module 'numpy' from '/Users/ivandominguez/anaconda3/lib/python3.11/site-packages/numpy/__init__.py'>)\n",
      "    #   $16load_method.2 = getattr(value=$4load_global.0, attr=empty_like)  :: Function(<built-in function empty_like>)\n",
      "    #   del $4load_global.0\n",
      "    #   out = call $16load_method.2(x, func=$16load_method.2, args=[Var(x, 2090011437.py:1)], kws=(), vararg=None, varkwarg=None, target=None)  :: (Array(float64, 1, 'C', False, aligned=True), omitted(default=None)) -> array(float64, 1d, C)\n",
      "    #   del $16load_method.2\n",
      "\n",
      "    out = np.empty_like(x)\n",
      "\n",
      "    # --- LINE 4 --- \n",
      "    #   $56load_global.5 = global(range: <class 'range'>)  :: Function(<class 'range'>)\n",
      "    #   $70load_attr.8 = getattr(value=x, attr=shape)  :: UniTuple(int64 x 1)\n",
      "    #   $const80.9.1 = const(int, 0)  :: Literal[int](0)\n",
      "    #   $82binary_subscr.10 = static_getitem(value=$70load_attr.8, index=0, index_var=$const80.9.1, fn=<built-in function getitem>)  :: int64\n",
      "    #   del $const80.9.1\n",
      "    #   del $70load_attr.8\n",
      "    #   $96call.11 = call $56load_global.5($82binary_subscr.10, func=$56load_global.5, args=[Var($82binary_subscr.10, 2090011437.py:4)], kws=(), vararg=None, varkwarg=None, target=None)  :: (int64,) -> range_state_int64\n",
      "    #   del $82binary_subscr.10\n",
      "    #   del $56load_global.5\n",
      "    #   $106get_iter.12 = getiter(value=$96call.11)  :: range_iter_int64\n",
      "    #   del $96call.11\n",
      "    #   $phi108.0 = $106get_iter.12  :: range_iter_int64\n",
      "    #   del $106get_iter.12\n",
      "    #   jump 108\n",
      "    # label 108\n",
      "    #   $108for_iter.1 = iternext(value=$phi108.0)  :: pair<int64, bool>\n",
      "    #   $108for_iter.2 = pair_first(value=$108for_iter.1)  :: int64\n",
      "    #   $108for_iter.3 = pair_second(value=$108for_iter.1)  :: bool\n",
      "    #   del $108for_iter.1\n",
      "    #   $phi110.1 = $108for_iter.2  :: int64\n",
      "    #   del $108for_iter.2\n",
      "    #   branch $108for_iter.3, 110, 196\n",
      "    # label 110\n",
      "    #   del $108for_iter.3\n",
      "    #   i = $phi110.1  :: int64\n",
      "    #   del $phi110.1\n",
      "\n",
      "    for i in range(x.shape[0]):\n",
      "\n",
      "        # --- LINE 5 --- \n",
      "        #   $const112.2.2 = const(int, 2)  :: Literal[int](2)\n",
      "        #   $118binary_subscr.5 = getitem(value=x, index=i, fn=<built-in function getitem>)  :: float64\n",
      "        #   $132binary_subscr.8 = getitem(value=y, index=i, fn=<built-in function getitem>)  :: float64\n",
      "        #   $binop_sub142.9 = $118binary_subscr.5 - $132binary_subscr.8  :: float64\n",
      "        #   del $132binary_subscr.8\n",
      "        #   del $118binary_subscr.5\n",
      "        #   $binop_mul146.10 = $const112.2.2 * $binop_sub142.9  :: float64\n",
      "        #   del $const112.2.2\n",
      "        #   del $binop_sub142.9\n",
      "        #   $154binary_subscr.13 = getitem(value=x, index=i, fn=<built-in function getitem>)  :: float64\n",
      "        #   $168binary_subscr.16 = getitem(value=y, index=i, fn=<built-in function getitem>)  :: float64\n",
      "        #   $binop_add178.17 = $154binary_subscr.13 + $168binary_subscr.16  :: float64\n",
      "        #   del $168binary_subscr.16\n",
      "        #   del $154binary_subscr.13\n",
      "        #   $binop_truediv182.18 = $binop_mul146.10 / $binop_add178.17  :: float64\n",
      "        #   del $binop_mul146.10\n",
      "        #   del $binop_add178.17\n",
      "        #   out[i] = $binop_truediv182.18  :: (Array(float64, 1, 'C', False, aligned=True), int64, float64) -> none\n",
      "        #   del i\n",
      "        #   del $binop_truediv182.18\n",
      "        #   jump 108\n",
      "\n",
      "        out[i] = 2 * (x[i] - y[i]) / (x[i] + y[i])\n",
      "\n",
      "    # --- LINE 6 --- \n",
      "    # label 196\n",
      "    #   del y\n",
      "    #   del x\n",
      "    #   del $phi110.1\n",
      "    #   del $phi108.0\n",
      "    #   del $108for_iter.3\n",
      "    #   $198return_value.1 = cast(value=out)  :: array(float64, 1d, C)\n",
      "    #   del out\n",
      "    #   return $198return_value.1\n",
      "\n",
      "    return out\n",
      "\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "frac_diff2.inspect_types() # pretty=True falla por culpa de la version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9TuRpbTGJCe"
   },
   "source": [
    "If you expand out line 5 in the float32 version of the function, you will see the following bit of Numba IR:\n",
    "\n",
    "```\n",
    "$const28.2 = const(float, 2.0) :: float64\n",
    "$28.5 = getitem(value=x, index=i) :: float32\n",
    "$28.8 = getitem(value=y, index=i) :: float32\n",
    "$28.9 = $28.5 - $28.8 :: float32\n",
    "del $28.8\n",
    "del $28.5\n",
    "$28.10 = $const28.2 * $28.9 :: float64\n",
    "```\n",
    "\n",
    "Notice that the constant `2` has been typed as a float64 value.  Later, this causes the multiplication `2 * (x[i] - y[i]` to promote up to float64, and then the rest of the calculation becomes float64.  This is a situation where Numba is being overly conservative (and should be fixed at some point), but we can tweak this behavior by casting the constant to the type we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "u-KAXl9qGJCe"
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True, error_model='numpy')\n",
    "def frac_diff3(x, y):\n",
    "    out = np.empty_like(x)\n",
    "    dt = x.dtype # Cast the constant using the dtype of the input\n",
    "    for i in range(x.shape[0]):\n",
    "        # Could also use np.float32(2) to always use same type, regardless of input\n",
    "        out[i] = dt.type(2) * (x[i] - y[i]) / (x[i] + y[i])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mJQgYBBaGJCf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6666667 , -0.66662216, -0.66657776, ..., -0.400032  ,\n",
       "       -0.40001604, -0.4       ], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frac_diff3(x32, y32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZTt42JO0GJCf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23 μs ± 34.1 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "2.27 μs ± 60.9 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit frac_diff3(x32, y32)\n",
    "%timeit frac_diff3(x64, y64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqGc9LQkGJCf"
   },
   "source": [
    "Now our float32 version is nice and speedy (and 6x faster than what we started with, if we only care about float32)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNwieXpGJCf"
   },
   "source": [
    "## SIMD and Reductions\n",
    "\n",
    "The autovectorizer can also optimize reduction loops, but only with permission.  Normally, compilers are very careful not to reorder floating point instructions because floating point arithmetic is approximate, so mathematically allowed transformations do not always give the same result.  For example, it is not generally true for floating point numbers that:\n",
    "\n",
    "```\n",
    "(a + (b + c)) == ((a + b) + c)\n",
    "```\n",
    "\n",
    "For many situations, the round-off error that causes the difference between the left and the right is not important, so changing the order of additions is acceptable for a performance increase.\n",
    "\n",
    "To allow reordering of operations, we need to tell Numba to enable `fastmath` optimizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "bHatnptAGJCf"
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def do_sum(A):\n",
    "    acc = 0.\n",
    "    # without fastmath, this loop must accumulate in strict order\n",
    "    for x in A:\n",
    "        acc += x**2\n",
    "    return acc\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def do_sum_fast(A):\n",
    "    acc = 0.\n",
    "    # with fastmath, the reduction can be vectorized as floating point\n",
    "    # reassociation is permitted.\n",
    "    for x in A:\n",
    "        acc += x**2\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "SCmfN2whGJCf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tfmul\ts1, s1, s1\n",
      "\tfmul\ts2, s2, s2\n",
      "\tfmul\ts3, s3, s3\n",
      "\tfmul\ts4, s4, s4\n",
      "\tfmul\ts1, s1, s1\n"
     ]
    }
   ],
   "source": [
    "do_sum(x32)\n",
    "find_instr(do_sum, keyword='fmul')  # Notación para el M4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "j1-hxR1SGJCf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tfmul\ts4, s4, s4\n",
      "\tfmul\ts5, s5, s5\n",
      "\tfmul\ts6, s6, s6\n",
      "\tfmul\ts7, s7, s7\n",
      "\tfmul\ts1, s1, s1\n"
     ]
    }
   ],
   "source": [
    "do_sum_fast(x32)\n",
    "find_instr(do_sum_fast, keyword='fmul') # Notación para el M4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQLaoMX9GJCg"
   },
   "source": [
    "The fast version is 4x faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "LG5mkJ6kGJCg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.56 μs ± 260 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      "1.87 μs ± 8.83 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit do_sum(x32)\n",
    "%timeit do_sum_fast(x32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYKgsPvoGJCg"
   },
   "source": [
    "## SIMD and Special Functions\n",
    "\n",
    "If you follow the above guidelines, SIMD autovectorization will work for all basic math operations (`+`,`-`,`*`,`\\`), but generally will not work for function calls in the loop, unless LLVM can inline the function and there is only basic math in the function body.\n",
    "\n",
    "However, we build Numba (if you get conda packages from Anaconda or wheels from PyPI) using a patched version of LLVM that supports vectorization of special math functions when Intel SVML (\"Short Vector Math Library\") is present.  This library comes with the Intel compiler, and is also freely redistributable.  We've installed it in the current conda environment using `conda install -c numba icc_rt`, as we can verify here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ! conda install -c numba icc_rt no funciona en esta arquitectura\n",
    "# su equivalente parece ser el siguiente\n",
    "\n",
    "! conda install -c conda-forge numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0EBReNchGJCl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /Users/ivandominguez/anaconda3:\n",
      "#\n",
      "# Name                     Version          Build            Channel\n",
      "libllvm14                  14.0.6           h19fdd8a_4\n",
      "libllvm20                  20.1.8           h1701f07_0\n",
      "llvm-openmp                20.1.8           he822017_0\n",
      "llvmlite                   0.45.1           py311hc8eb11b_0\n",
      "numba                      0.62.1           py311h471b49b_0\n"
     ]
    }
   ],
   "source": [
    "# en este caso el equivalente para mac sería:\n",
    "\n",
    "! conda list \"numba|llvm|libblas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAhpeD_qGJCl"
   },
   "source": [
    "Thanks to this library, we can still get SIMD vectorization in a function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "FNTG06ROGJCl"
   },
   "outputs": [],
   "source": [
    "SQRT_2PI = np.sqrt(2 * np.pi)\n",
    "\n",
    "@jit(nopython=True, error_model='numpy', fastmath=True)\n",
    "def kde(x, means, widths):\n",
    "    '''Compute value of gaussian kernel density estimate.\n",
    "\n",
    "    x - location of evaluation\n",
    "    means - array of kernel means\n",
    "    widths - array of kernel widths\n",
    "    '''\n",
    "    n = means.shape[0]\n",
    "    acc = 0.\n",
    "    for i in range(n):\n",
    "        acc += np.exp( -0.5 * ((x - means[i]) / widths[i])**2 ) / widths[i]\n",
    "    return acc / SQRT_2PI / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "WhK8AkoSGJCl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48401212391619747"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The distribution we are approximating is flat between -1 and 1, so we expect a KDE value of ~0.5 everywhere\n",
    "means = np.random.uniform(-1, 1, size=10000)\n",
    "# These widths are not selected in any reasonable way.  Consult your local statistician before approximating a PDF.\n",
    "widths = np.random.uniform(0.1, 0.3, size=10000)\n",
    "\n",
    "kde(0.4, means, widths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OQO52OYGJCm"
   },
   "source": [
    "We can see that SIMD instructions were generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "FHko3pe_GJCm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tfsub.2d\tv0, v4, v0\n",
      "\tfsub.2d\tv1, v4, v1\n",
      "\tfsub.2d\tv2, v4, v2\n",
      "\tfsub.2d\tv3, v4, v3\n",
      "\tfsub\td0, d1, d0\n"
     ]
    }
   ],
   "source": [
    "find_instr(kde, 'fsub')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oB2QcrJDGJCm"
   },
   "source": [
    "We can also see that calls to the special Intel SVML functions for `exp` were generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "yv-5iUC_GJCm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t.build_version macos, 16, 0\n",
      "\t.globl\t__ZN8__main__3kdeB3v19B64c8tJTIeFIjxB2IKSgI4CrvQCk0Z4yRYcWsCAgXrKFt1X1eogKfVaTWDAFkEM0AQAEd5ArrayIdLi1E1C7mutable7alignedE5ArrayIdLi1E1C7mutable7alignedE\n",
      "__ZN8__main__3kdeB3v19B64c8tJTIeFIjxB2IKSgI4CrvQCk0Z4yRYcWsCAgXrKFt1X1eogKfVaTWDAFkEM0AQAEd5ArrayIdLi1E1C7mutable7alignedE5ArrayIdLi1E1C7mutable7alignedE:\n",
      "\tmov\tx19, x7\n",
      "\tmov\tx20, x0\n"
     ]
    }
   ],
   "source": [
    "find_instr(kde, keyword='v') # equivalente para mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "jjI0Q-G9GJCm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.3 μs ± 1.22 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit kde(0.4, means, widths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2rxfUMkGJCm"
   },
   "source": [
    "If we recompile the function (which is possible since the `.py_func` attribute holds the original Python function) with the extra flags to allow division and reductions to work, this stops all autovectorization of the loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "IsWAkH04GJCm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48401212391619675"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slow_kde = jit(nopython=True)(kde.py_func)\n",
    "\n",
    "slow_kde(0.4, means, widths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWN6V2h1GJCm"
   },
   "source": [
    "Note that we get a slightly different answer, both due to the different order of operations, and the small differences in SVML `exp` compared to the default `exp`.  We also see that there is no SIMD or calls to SVML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "rvII8oBiGJCn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tfsub\td0, d8, d0\n",
      "---\n",
      "\t.build_version macos, 16, 0\n",
      "\t.globl\t__ZN8__main__3kdeB3v21B38c8tJTIeFIjxB2IKSgI4CrvQClQZ6FczSBAA_3dEd5ArrayIdLi1E1C7mutable7alignedE5ArrayIdLi1E1C7mutable7alignedE\n",
      "__ZN8__main__3kdeB3v21B38c8tJTIeFIjxB2IKSgI4CrvQClQZ6FczSBAA_3dEd5ArrayIdLi1E1C7mutable7alignedE5ArrayIdLi1E1C7mutable7alignedE:\n",
      "\tmov\tx20, x7\n",
      "\tmov\tx21, x1\n"
     ]
    }
   ],
   "source": [
    "find_instr(slow_kde, keyword='fsub') # equivalente para mac\n",
    "print('---')\n",
    "find_instr(slow_kde, keyword='v') # equivalente para mac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5mYxpXvGJCn"
   },
   "source": [
    "And the function is much slower than the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "i3-YVdLeGJCn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.2 μs ± 788 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "22.7 μs ± 42.2 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit kde(0.4, means, widths)\n",
    "%timeit slow_kde(0.4, means, widths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RftddODkGJCn"
   },
   "source": [
    "And only the SIMD vectorized version is faster than doing this in pure NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "5rtPSMy8GJCn"
   },
   "outputs": [],
   "source": [
    "def numpy_kde(x, means, widths):\n",
    "    acc = (np.exp( -0.5 * ((x - means) / widths)**2 ) / widths).mean()\n",
    "    # .mean() already divides by n\n",
    "    return acc / SQRT_2PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Rb83oP66GJCn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48401212391619775"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_kde(0.4, means, widths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "OLH0NTtZGJCn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 μs ± 526 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit numpy_kde(0.4, means, widths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmjX5ixxGJCo"
   },
   "source": [
    "Why is NumPy as fast as it is?  In this case, it is because the Anaconda build of NumPy uses MKL to accelerate (with SIMD and threads) many of the individial ufuncs, so it is only when Numba can combine all the operations together that the speed boost emerges.\n",
    "\n",
    "Incidentally, although we wrote out the iteration for `kde` as a for loop to highlight what was going on, you still get the benefit of SIMD in Numba when compiling array expressions.  We could have compiled `numpy_kde` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "umwua7DTGJCo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48401212391619736"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numba_numpy_kde = jit(nopython=True, error_model='numpy', fastmath=True)(numpy_kde)\n",
    "\n",
    "numba_numpy_kde(0.4, means, widths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "cV2VNUgJGJCo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tfsub\td0, d1, d0\n",
      "\tfsub\td0, d2, d0\n",
      "\tfsub\td0, d1, d0\n",
      "\tfsub\td0, d2, d0\n",
      "\tfsub\td0, d2, d0\n",
      "---\n",
      "\t.build_version macos, 16, 0\n",
      "\t.globl\t__ZN8__main__9numpy_kdeB3v22B64c8tJTIeFIjxB2IKSgI4CrvQCk0Z4yRYcWsCAgXrKFt1X1eogKfVaTWDAFkEM0AQAEd5ArrayIdLi1E1C7mutable7alignedE5ArrayIdLi1E1C7mutable7alignedE\n",
      "__ZN8__main__9numpy_kdeB3v22B64c8tJTIeFIjxB2IKSgI4CrvQCk0Z4yRYcWsCAgXrKFt1X1eogKfVaTWDAFkEM0AQAEd5ArrayIdLi1E1C7mutable7alignedE5ArrayIdLi1E1C7mutable7alignedE:\n",
      "\tmov\tx24, x7\n",
      "\tmov\tx20, x6\n"
     ]
    }
   ],
   "source": [
    "find_instr(numba_numpy_kde, keyword='fsub')\n",
    "print('---')\n",
    "find_instr(numba_numpy_kde, keyword='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGsnUvDhGJCo"
   },
   "source": [
    "And it is nearly as fast as our manual looping version, and 2x faster than NumPy alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "9QjtOgzCGJCo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.1 μs ± 490 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "22.4 μs ± 46.1 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "33.4 μs ± 221 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit kde(0.4, means, widths)\n",
    "%timeit numba_numpy_kde(0.4, means, widths)\n",
    "%timeit numpy_kde(0.4, means, widths)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
